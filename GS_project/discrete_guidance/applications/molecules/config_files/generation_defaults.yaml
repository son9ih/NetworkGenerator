# Specify general configuration
state: 'generation'
device: 'cuda'
num_gpus: 1
eps_ratio: 1e-9 # Will be parsed as string!
base_dir: '.'

# Relative path to the train folder (with the model weights etc.) used to load models for generation:
# trained_run_folder_dir: './trained/CTFM'
trained_run_folder_dir: './trained/2025-02-09/no_overrides'
# /home/son9ih/discrete_guidance/applications/molecules/trained/2025-02-09/no_overrides/models_saved

# Specify the sampler specific configurations
sampler:
  noise: 30.0                 # Stochasticity '\eta'
  x1_temp: 1.0                # Temperature for unconditional model
  do_purity_sampling: False   # Should we use purity sampling?
  purity_temp: 1.0            # Temperature for purity sampling (only relevant if do_purity_sampling=True)     
  argmax_final: True          # Should we use argmaxing in the last generation step?
  max_t: 0.98                 # Maximal time point during generation
  dt: 0.001                   # Time step for Euler-method, 1000 steps per generation 
  sample_zero_time_pads: True # Should we sample SMILES padding at time zero?
  guide_temp: 0.5             # Guidance temperature, this is lambda in the paper
  grad_approx: False          # Use Taylor-approximated guidance (TAG) or not
  seed: 30                    # Seed to be used to initialize random states of generation
  batch_size: 100             # How many samples to draw in parallel, generate 100 samples in parallel
  max_iterations: 100000      # The maximal number of times the generator is invoked, 
  # 내가 args에 인자로 넣어준 1000개의 유효한 샘플을 뽑기 전까지 필요하면 100000번동안 한번에 100개의 후보 샘플들을 sampling하겠다
